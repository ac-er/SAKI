{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75821b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WarehouseWorld:\n",
    "\n",
    "    def __init__(self, size_0=2, size_1=2, prob_1=1, prob_2=1, prob_3=1):\n",
    "        \"\"\"\n",
    "            size_0: first dimension of the warehouse\n",
    "            size_1: second dimension of the warehouse\n",
    "            prob_1: frequency of item 1\n",
    "            prob_2: frequency of item 2\n",
    "            prob_3; frequency of item 3\n",
    "        \"\"\"\n",
    "        self.size_0 = size_1\n",
    "        self.size_1 = size_0\n",
    "        self.prob_1 = prob_1\n",
    "        self.prob_2 = prob_2\n",
    "        self.prob_3 = prob_3\n",
    "\n",
    "        \"\"\"\n",
    "        Warehouse State\n",
    "            0 = no item\n",
    "            1 = white item\n",
    "            2 = red item\n",
    "            3 = blue item\n",
    "            example: warehouse is full with with white items\n",
    "             [1. 1.]\n",
    "             [1. 1.]   \n",
    "        \"\"\"\n",
    "        self.map = np.zeros((size_0, size_1)).astype(int)\n",
    "\n",
    "        \"\"\"\n",
    "        Goal State:\n",
    "            int goal_move: 0 = store, 1 = restore\n",
    "            int goal_item: 1..3 (white, red, blue)\n",
    "            list goal_distribution: probabilities of the items [white, red, blue]\n",
    "            restore_item: The restore item is calculated depending the warehouse state and the goal distribution\n",
    "        \"\"\"\n",
    "        self.goal_move = None\n",
    "        self.goal_item = None\n",
    "        self.goal_distribution = np.array([self.prob_1, self.prob_2, self.prob_3]) / (\n",
    "                self.prob_1 + self.prob_2 + self.prob_3)\n",
    "        self.create_new_task()\n",
    "\n",
    "        \"\"\"\n",
    "            Punishment of -1 for each field away from the entry at the top left (perfect spot)\n",
    "            Entry at top left has no punishment\n",
    "            No diagonal moves are possible\n",
    "            Punishment for a task that doesnt solve the goal: - size_0 + size_1\n",
    "            => to optimize the reward the robot has to find a solution with a short distance but also fulfill the task\n",
    "        \"\"\"\n",
    "        self.reward = 0\n",
    "\n",
    "        # helpers to decode/encode the state to a int number\n",
    "        self.warehouse_decode = None\n",
    "        self.goal_item_decode = None\n",
    "        self.goal_move_decode = None\n",
    "        self.generate_helpers()\n",
    "\n",
    "    def create_new_task(self):\n",
    "        # warehouse is full\n",
    "        if np.all(self.map):\n",
    "            self.goal_move = 1\n",
    "\n",
    "        # warehouse is empty\n",
    "        elif not np.any(self.map):\n",
    "            self.goal_move = 0\n",
    "\n",
    "        # warehouse is not empty nor full\n",
    "        else:\n",
    "            self.goal_move = np.random.binomial(1, 0.5)\n",
    "\n",
    "        # store\n",
    "        if self.goal_move == 0:\n",
    "            self.goal_item = np.random.random()\n",
    "            if self.goal_item < self.goal_distribution[0]:\n",
    "                self.goal_item = 1\n",
    "            elif self.goal_item < (self.goal_distribution[0] + self.goal_distribution[1]):\n",
    "                self.goal_item = 2\n",
    "            else:\n",
    "                self.goal_item = 3\n",
    "\n",
    "        # restore\n",
    "        else:\n",
    "\n",
    "            count = np.bincount(self.map.flatten(), minlength=4)\n",
    "            distribution = count[1:] / (np.sum(count) - count[0])\n",
    "            distribution = np.multiply(distribution > 0, self.goal_distribution)\n",
    "            distribution = distribution / np.sum(distribution)\n",
    "            self.goal_item = np.random.random()\n",
    "            if self.goal_item < distribution[0]:\n",
    "                self.goal_item = 1\n",
    "            elif self.goal_item < (distribution[0] + distribution[1]):\n",
    "                self.goal_item = 2\n",
    "            else:\n",
    "                self.goal_item = 3\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "            action = place to store/restore the item\n",
    "        \"\"\"\n",
    "        index = np.unravel_index(action, self.map.shape)\n",
    "        completed = False\n",
    "\n",
    "        # store\n",
    "        if self.goal_move == 0 and self.map[index] == 0:\n",
    "            self.map[index] = self.goal_item\n",
    "            completed = True\n",
    "\n",
    "        # restore\n",
    "        if self.goal_move == 1 and self.map[index] == self.goal_item:\n",
    "            self.map[index] = 0\n",
    "            completed = True\n",
    "\n",
    "        if completed:\n",
    "            self.create_new_task()\n",
    "            reward = - index[0] - index[1]\n",
    "        else:\n",
    "            reward = - index[0] - index[1] - self.size_0 - self.size_1\n",
    "\n",
    "        return reward, completed\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.goal_move == 1:\n",
    "            goal = 'Restore'\n",
    "        else:\n",
    "            goal = 'Store'\n",
    "\n",
    "        if self.goal_item == 1:\n",
    "            goal_item = 'White(1)'\n",
    "        elif self.goal_item == 2:\n",
    "            goal_item = 'Red(2)'\n",
    "        else:\n",
    "            goal_item = 'Blue(3)'\n",
    "\n",
    "        print(\"Reward:\", self.reward)\n",
    "        print(\"Goal:\", goal, goal_item)\n",
    "        print(self.map)\n",
    "\n",
    "    def generate_helpers(self):\n",
    "        # different warehouse states\n",
    "        items = []\n",
    "        for i in range(self.size_0 * self.size_1):\n",
    "            items.append([0, 1, 2, 3])\n",
    "        self.warehouse_decode = np.array(np.meshgrid(*items)).T.reshape(-1, self.size_0 * self.size_1)\n",
    "\n",
    "        # different goal_states\n",
    "        self.goal_item_decode = np.array([1, 2, 3])\n",
    "        self.goal_move_decode = np.array([0, 1])\n",
    "\n",
    "    def state_to_int(self):\n",
    "        warehouse = self.map.astype(int).flatten()\n",
    "        warehouse = np.argwhere((self.warehouse_decode == warehouse).all(axis=1))\n",
    "\n",
    "        goal_item = self.goal_item\n",
    "        goal_item = np.argwhere(self.goal_item_decode == goal_item)\n",
    "\n",
    "        goal_move = self.goal_move\n",
    "        goal_move = np.argwhere(self.goal_move_decode == goal_move)\n",
    "\n",
    "        result = warehouse[0, 0]\n",
    "        factor = self.warehouse_decode.shape[0]\n",
    "\n",
    "        result += goal_item[0, 0] * factor\n",
    "        factor *= self.goal_item_decode.shape[0]\n",
    "\n",
    "        result += goal_move[0, 0] * factor\n",
    "        return result\n",
    "\n",
    "    def int_to_state(self, state_number):\n",
    "        factor = self.warehouse_decode.shape[0] * self.goal_item_decode.shape[0]\n",
    "        goal_move = int(state_number // factor)\n",
    "        state_number = state_number % factor\n",
    "\n",
    "        factor /= self.goal_item_decode.shape[0]\n",
    "        goal_item = int(state_number // factor)\n",
    "        state_number = state_number % factor\n",
    "\n",
    "        warehouse = int(state_number)\n",
    "\n",
    "        self.map = np.copy(self.warehouse_decode[warehouse].reshape((self.size_0, self.size_1)))\n",
    "        self.goal_item = np.copy(self.goal_item_decode[goal_item])\n",
    "        self.goal_move = np.copy(self.goal_move_decode[goal_move])\n",
    "\n",
    "    def give_next_state_probabilities(self, action):\n",
    "        \"\"\"\n",
    "            action = place to store/restore the item\n",
    "        \"\"\"\n",
    "        reward, completed = self.move(action)\n",
    "\n",
    "        if not completed:\n",
    "            return [(1, self.state_to_int())], reward\n",
    "        else:\n",
    "            next_states = []\n",
    "\n",
    "            # warehouse is empty\n",
    "            if not np.any(self.map):\n",
    "                self.goal_move = 0\n",
    "                self.goal_item = 1\n",
    "                next_states.append((self.goal_distribution[0], self.state_to_int()))\n",
    "                self.goal_item = 2\n",
    "                next_states.append((self.goal_distribution[1], self.state_to_int()))\n",
    "                self.goal_item = 3\n",
    "                next_states.append((self.goal_distribution[2], self.state_to_int()))\n",
    "\n",
    "            # warehouse is full\n",
    "            elif np.all(self.map):\n",
    "                self.goal_move = 1\n",
    "                count = np.bincount(self.map.flatten(), minlength=4)\n",
    "                distribution = count[1:] / (np.sum(count) - count[0])\n",
    "                distribution = np.multiply(distribution > 0, self.goal_distribution)\n",
    "                distribution = distribution / np.sum(distribution)\n",
    "                self.goal_item = 1\n",
    "                next_states.append((distribution[0], self.state_to_int()))\n",
    "                self.goal_item = 2\n",
    "                next_states.append((distribution[1], self.state_to_int()))\n",
    "                self.goal_item = 3\n",
    "                next_states.append((distribution[2], self.state_to_int()))\n",
    "\n",
    "            # both possible\n",
    "            else:\n",
    "                self.goal_move = 0\n",
    "                self.goal_item = 1\n",
    "                next_states.append((self.goal_distribution[0] / 2, self.state_to_int()))\n",
    "                self.goal_item = 2\n",
    "                next_states.append((self.goal_distribution[1] / 2, self.state_to_int()))\n",
    "                self.goal_item = 3\n",
    "                next_states.append((self.goal_distribution[2] / 2, self.state_to_int()))\n",
    "\n",
    "                self.goal_move = 1\n",
    "                count = np.bincount(self.map.flatten(), minlength=4)\n",
    "                distribution = count[1:] / (np.sum(count) - count[0])\n",
    "                distribution = np.multiply(distribution > 0, self.goal_distribution)\n",
    "                distribution = distribution / np.sum(distribution)\n",
    "                self.goal_item = 1\n",
    "                next_states.append((distribution[0] / 2, self.state_to_int()))\n",
    "                self.goal_item = 2\n",
    "                next_states.append((distribution[1] / 2, self.state_to_int()))\n",
    "                self.goal_item = 3\n",
    "                next_states.append((distribution[2] / 2, self.state_to_int()))\n",
    "\n",
    "            return next_states, reward\n",
    "\n",
    "    def build_transtion_and_reward_matrix(self):\n",
    "        state_space = self.warehouse_decode.shape[0] * self.goal_item_decode.shape[0] * self.goal_move_decode.shape[0]\n",
    "        actionspace = self.size_0 * self.size_1\n",
    "\n",
    "        transitions = np.zeros((actionspace, state_space, state_space)).astype(np.float16)\n",
    "        rewards = np.zeros((state_space, actionspace)).astype(np.float16)\n",
    "        for a in range(actionspace):\n",
    "            for s in range(state_space):\n",
    "                self.int_to_state(s)\n",
    "                states_reward = self.give_next_state_probabilities(a)\n",
    "                rewards[s,a] = states_reward[1]\n",
    "                for next_s in states_reward[0]:\n",
    "                    transitions[a,s,next_s[1]] = next_s[0]\n",
    "\n",
    "        return transitions, rewards\n",
    "\n",
    "\n",
    "    def test_policies(self, pol_1, pol_2, numb_moves):\n",
    "        self.int_to_state(0)\n",
    "        self.reward = 0\n",
    "        goal_items = []\n",
    "        goal_moves = []\n",
    "\n",
    "        #pol_1\n",
    "        for i in range(numb_moves):\n",
    "            action = pol_1[self.state_to_int()]\n",
    "            reward, completed = self.move(action)\n",
    "\n",
    "            if completed:\n",
    "                goal_items.append(self.goal_item)\n",
    "                goal_moves.append(self.goal_move)\n",
    "            self.reward += reward\n",
    "        reward_1 = self.reward\n",
    "\n",
    "        self.int_to_state(0)\n",
    "        self.reward = 0\n",
    "        \n",
    "        # pol_2\n",
    "        for i in range(numb_moves):\n",
    "            action = pol_2[self.state_to_int()]\n",
    "            reward, completed = self.move(action)\n",
    "\n",
    "            if completed:\n",
    "                self.goal_item = goal_items.pop(0)\n",
    "                self.goal_move = goal_moves.pop(0)\n",
    "            self.reward += reward\n",
    "        reward_2 = self.reward\n",
    "\n",
    "        return reward_1, reward_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da4022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build environment and transition/reward matrix\n",
    "env= WarehouseWorld(size_0=1, size_1=5, prob_1=5, prob_2=90, prob_3=5)\n",
    "transitions, rewards = env.build_transtion_and_reward_matrix()\n",
    "\n",
    "print(transitions.shape)\n",
    "print(rewards.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5358566e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \tIteration\t\tV-variation\n",
      "    1\t\t  6.059814453125\n",
      "    2\t\t  9.435956682858148\n",
      "    3\t\t  3.630806276081387\n",
      "    4\t\t  2.6502466817330514\n",
      "    5\t\t  2.1160581534854828\n",
      "    6\t\t  1.6908835960944941\n",
      "    7\t\t  1.3517934904775188\n",
      "    8\t\t  1.0808908765747276\n",
      "    9\t\t  0.86402970626051\n",
      "    10\t\t  0.690864656256494\n",
      "    11\t\t  0.5522449926616417\n",
      "    12\t\t  0.44156449498619565\n",
      "    13\t\t  0.35296449336534863\n",
      "    14\t\t  0.2822229766049702\n",
      "    15\t\t  0.22559428273805082\n",
      "    16\t\t  0.18038006873977253\n",
      "    17\t\t  0.1441860370455359\n",
      "    18\t\t  0.11528764933604663\n",
      "    19\t\t  0.0921544651615065\n",
      "    20\t\t  0.07368431973119982\n",
      "    21\t\t  0.05889895839266046\n",
      "    22\t\t  0.047093982974693915\n",
      "    23\t\t  0.03764409763189036\n",
      "    24\t\t  0.030099120624896614\n",
      "    25\t\t  0.0240593673748819\n",
      "    26\t\t  0.019237127536811727\n",
      "    27\t\t  0.01537692666273216\n",
      "    28\t\t  0.012294890459209284\n",
      "    29\t\t  0.009827722843326114\n",
      "    30\t\t  0.007857911202805212\n",
      "    31\t\t  0.006281079146170043\n",
      "    32\t\t  0.0050221256493045985\n",
      "    33\t\t  0.004014335173998518\n",
      "    34\t\t  0.003209711709274643\n",
      "    35\t\t  0.002565612043525789\n",
      "    36\t\t  0.0020513627481264507\n",
      "    37\t\t  0.0016397072653475675\n",
      "    38\t\t  0.001311042827694564\n",
      "    39\t\t  0.0010479477449578667\n",
      "    40\t\t  0.0008378943464038002\n",
      "    41\t\t  0.0006697471828758239\n",
      "    42\t\t  0.000535500152018642\n",
      "    43\t\t  0.0004280358213080149\n",
      "    44\t\t  0.00034223772827601806\n",
      "    45\t\t  0.0002735566508462739\n",
      "    46\t\t  0.00021872282594159742\n",
      "    47\t\t  0.00017482855706418832\n"
     ]
    }
   ],
   "source": [
    "import mdptoolbox as mdp\n",
    "\n",
    "# Calucalte better policy\n",
    "model = mdp.mdp.PolicyIterationModified(transitions, rewards, 0.98)\n",
    "model.setVerbose()\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ce36fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy (6144,)\n",
      "Greedy_Policy (6144,)\n",
      "Same Policy: False\n"
     ]
    }
   ],
   "source": [
    "# Check result being different\n",
    "policy = np.array(model.policy)\n",
    "greedy_policy = np.argmax(rewards, axis=1)\n",
    "\n",
    "print(\"Policy\", policy.shape)\n",
    "print(\"Greedy_Policy\",greedy_policy.shape)\n",
    "print(\"Same Policy:\",np.array_equal(policy, greedy_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f41234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average additional distance per task\n",
      "policy: 0.8945\n",
      "greedy: 0.9575\n"
     ]
    }
   ],
   "source": [
    "#Evaluate new policy\n",
    "numb_tasks = 10000\n",
    "reward_greedy, reward_policy = env.test_policies(greedy_policy, policy, numb_tasks)\n",
    "print(\"Average additional distance per task\")\n",
    "print(\"policy:\", -reward_policy/numb_tasks)\n",
    "print(\"greedy:\", -reward_greedy/numb_tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
